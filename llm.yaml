apiVersion: apps/v1
kind: Deployment
metadata:
  name: typhoon-2-5-vllm
  labels:
    app: typhoon-vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: typhoon-vllm
  template:
    metadata:
      labels:
        app: typhoon-vllm
    spec:
      nodeSelector:
        kubernetes.io/hostname: ailprsvai02 # ระบุ node ที่มี L40S GPU
      containers:
      - name: ollama-server
        image: ollama/ollama:latest
        command: ["/bin/sh", "-c", "ollama serve"]
        env:
          - name: OLLAMA_MODELS
            value: "/root/.ollama"
          - name: OLLAMA_KEEP_ALIVE
            value: "-1"
          - name: OLLAMA_NUM_PARALLEL
            value: "1"
          - name: OLLAMA_MAX_LOADED_MODELS
            value: "1"
          - name: OLLAMA_CONTEXT_LENGTH
            value: "32768"
        ports:
        - containerPort: 11434
        lifecycle:
          postStart:
            exec:
              command:
                - /bin/sh
                - -c
                - |
                  until ollama list >/dev/null 2>&1; do sleep 2; done
                  ollama pull scb10x/typhoon2.5-qwen3-30b-a3b:latest || true
        resources:
          requests:
            nvidia.com/gpu: "1"
          limits:
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: model-cache
          mountPath: /root/.ollama
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: vllm-models-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: typhoon-vllm-service
spec:
  selector:
    app: typhoon-vllm
  ports:
  - port: 80
    targetPort: 11434
  type: NodePort
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-models-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 150Gi 
  storageClassName: ceph-rbd
